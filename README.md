# DTT

> [Deep Tri-Training for Semi-Supervised Image Segmentation](https://ieeexplore.ieee.org/document/9804753)
>
> by Shan An; Haogang Zhu; Jiaao Zhang; Junjie Ye; Siliang Wang; Jianqin Yin; Hong Zhang

## Introduction
Inspired by previous work in classification [11], DTT involves three models to make full use of unlabeled images appropriately.Concretely, three models are optimized circularly. One model is updated in each training step, where the pseudo labels in the unlabeled set are generated by the other two models through a voting-based integration. In our manner, joint training of the three models effectively reduces the risk of model degradation. The voting mechanism not only ensures the reliability of training data but also leads to robust segmentation results.

## Datasets
- [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/)
- [Cityscapes](https://www.cityscapes-dataset.com/)

## Methods and framework

 ![method](https://github.com/anshan-ar/DTT/blob/main/method.png)
- At the beginning of training, three networks with the same structure are initialized with different parameters respectively. 
- In the training stage, three models are optimized one by one. The figure illustrates the updating step of modelF(θk). The pseudo labels of unlabeled imagesXuused for training are voted by the other two models,i.e.,F(θi)andF(θj). The consistency rates of networks’ prediction for unlabeled images are calculated. The batches with a low consistency rate are filtered to avoid polluting the training set.
- In the testing stage, the final prediction results are also voted by the three well-trained models.

## Citation

Please consider citing this project in your publications if it helps your research.

```bibtex
@ARTICLE{9804753,
  author={An, Shan and Zhu, Haogang and Zhang, Jiaao and Ye, Junjie and Wang, Siliang and Yin, Jianqin and Zhang, Hong},
  journal={IEEE Robotics and Automation Letters}, 
  title={Deep Tri-Training for Semi-Supervised Image Segmentation}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/LRA.2022.3185768}}
```
